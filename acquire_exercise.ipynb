{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db10da63-89d4-41dd-81aa-6d81d61645fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157d91e-1323-4f6b-8e5f-89c3cfe36489",
   "metadata": {},
   "source": [
    "1. Codeup Blog Articles\n",
    "\n",
    "Visit Codeup's Blog and record the urls for at least 5 distinct blog posts. For each post, you should scrape at least the post's title and content.\n",
    "\n",
    "Encapsulate your work in a function named get_blog_articles that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adda713-13bc-4716-bf83-a51064e4d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'title': 'the title of the article',\n",
    "    'content': 'the full text content of the article'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b1eb5-37e6-497a-865b-54b814c0dbdb",
   "metadata": {},
   "source": [
    "write a function called get_blog_articles to get the codeup_blog_articles dictionary above from url = 'https://codeup.com/blog/' with \n",
    "headers = {'User-Agent': 'Codeup Data Science'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75aefe3b-2624-4b9e-899b-dc8cd337fc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://codeup.com/blog/'\n",
    "headers = {'User-Agent': 'Codeup Data Science'}\n",
    "response = requests.get(url, headers=headers)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b35c1de-b3f3-4a11-ad40-23b33b17609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d9950f5-2d19-4a55-ab4b-6b1820629ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Women in tech: Panelist Spotlight – Magdalena Rahn'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.h2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "393aae42-a9b8-42b2-9a1b-645942597649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"post-meta\"><span class=\"published\">Mar 28, 2023</span></p>,\n",
       " <p>Women in tech: Panelist Spotlight – Magdalena Rahn Codeup is hosting a Women in Tech Panel in honor of Women’s History...</p>,\n",
       " <p class=\"post-meta\"><span class=\"published\">Mar 20, 2023</span></p>,\n",
       " <p>Women in tech: Panelist Spotlight - Rachel Robbins-Mayhill Codeup is hosting a Women in Tech Panel in honor of Women’s...</p>,\n",
       " <p class=\"post-meta\"><span class=\"published\">Mar 13, 2023</span></p>,\n",
       " <p>Women in tech: Panelist Spotlight - Sarah Mellor  Codeup is hosting a Women in Tech Panel in honor of Women’s History...</p>,\n",
       " <p class=\"post-meta\"><span class=\"published\">Mar 6, 2023</span></p>,\n",
       " <p>Women in tech: Panelist Spotlight - Madeleine Capper Codeup is hosting a Women in Tech Panel in honor of Women’s...</p>,\n",
       " <p class=\"post-meta\"><span class=\"published\">Feb 16, 2023</span></p>,\n",
       " <p>Black excellence in tech: Panelist Spotlight - Wilmarie De La Cruz Mejia Codeup is hosting a Black Excellence in Tech...</p>,\n",
       " <p class=\"post-meta\"><span class=\"published\">Feb 13, 2023</span></p>,\n",
       " <p>Black excellence in tech: Panelist Spotlight - Stephanie Jones Codeup is hosting our second Black Excellence in Tech...</p>,\n",
       " <p style=\"text-align: center;\">Sign up for the Codeup Newsletter below and get notified on upcoming events, workshops and <em>Codeup</em>dates.</p>,\n",
       " <p style=\"text-align: center;\">Submit your email address to gain access to more information about our programs, financial aid, and benefits.</p>,\n",
       " <p><a href=\"tel:12108027289\" title=\"call Codeup\">(210) 802–7289</a></p>,\n",
       " <p><a href=\"/dallas\">Dallas</a> | <a href=\"/san-antonio\">San Antonio</a></p>,\n",
       " <p>© 2013-2022 Copyright. <a href=\"/privacy/\">Privacy Policy</a> | <a href=\"/student-complaint-policy/\" title=\"complaint policy\">Complaint Policy</a> | <a href=\"/codeup-news/inclusion-at-codeup-during-pride-month-and-always/\" title=\"DEI at Codeup\">Inclusion</a> | <a href=\"/accessibility/\">Accessibility</a> | <a href=\"/sitemap_index.xml\">Sitemap</a></p>,\n",
       " <p><span style=\"font-size: x-small;\">If you love researching data and reading the fine print, you’ll do well in our programs.</span></p>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2ad6666-2421-42ae-a274-0b138cb30fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_p_elements = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "168a3f95-2380-41cd-9f21-4a91b2c9130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "<a href=\"tel:12108027289\" title=\"call Codeup\">(210) 802–7289</a>\n",
      "<a href=\"/dallas\">Dallas</a>\n",
      "<a href=\"/privacy/\">Privacy Policy</a>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for element in list_of_p_elements:\n",
    "    print(element.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53ac8ce3-365e-43cb-8121-da6116f76076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 class=\"entry-title\"><a href=\"https://codeup.com/featured/women-in-tech-panelist-spotlight/\">Women in tech: Panelist Spotlight – Magdalena Rahn</a></h2>,\n",
       " <h2 class=\"entry-title\"><a href=\"https://codeup.com/featured/women-in-tech-rachel-robbins-mayhill/\">Women in tech: Panelist Spotlight – Rachel Robbins-Mayhill</a></h2>,\n",
       " <h2 class=\"entry-title\"><a href=\"https://codeup.com/codeup-news/women-in-tech-panelist-spotlight-sarah-mellor/\">Women in Tech: Panelist Spotlight – Sarah Mellor</a></h2>,\n",
       " <h2 class=\"entry-title\"><a href=\"https://codeup.com/events/women-in-tech-madeleine/\">Women in Tech: Panelist Spotlight – Madeleine Capper</a></h2>,\n",
       " <h2 class=\"entry-title\"><a href=\"https://codeup.com/codeup-news/panelist-spotlight-4/\">Black Excellence in Tech: Panelist Spotlight – Wilmarie De La Cruz Mejia</a></h2>,\n",
       " <h2 class=\"entry-title\"><a href=\"https://codeup.com/events/black-excellence-in-tech-panelist-spotlight-stephanie-jones/\">Black excellence in tech: Panelist Spotlight – Stephanie Jones</a></h2>,\n",
       " <h2 style=\"text-align: center;\">Git <strong>Codeup</strong>dates</h2>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faaca6dd-155d-429e-a4c9-5364b86b508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_h2_elements = soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f8f569d-9c69-4a11-9a8a-32b886b6818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_blog_articles = []\n",
    "\n",
    "for i in range(len(list_of_h2_elements)):\n",
    "    # Create a dictionary for each article\n",
    "    article_dict = {}\n",
    "    \n",
    "    # Extract title using regex\n",
    "    title_search = re.search('<a href=\".+\">(.+)</a>', str(list_of_h2_elements[i]))\n",
    "    if title_search is not None:\n",
    "        article_dict['title'] = title_search.group(1)\n",
    "    \n",
    "    # Extract content using regex\n",
    "    content_search = re.search('<p>(.+)</p>', str(list_of_p_elements[i*2+1])) # assumes every other p element is content\n",
    "    if content_search is not None:\n",
    "        article_dict['content'] = content_search.group(1)\n",
    "    \n",
    "    # Add the article to the list\n",
    "    codeup_blog_articles.append(article_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d163e87-f818-47ee-9da3-5fc90d2f0b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codeup_blog_articles():\n",
    "    url = 'https://codeup.com/blog/'\n",
    "    headers = {'User-Agent': 'Codeup Data Science'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    list_of_p_elements = soup.find_all('p')\n",
    "    list_of_h2_elements = soup.find_all('h2')\n",
    "    codeup_blog_articles = []\n",
    "\n",
    "    for i in range(len(list_of_h2_elements)):\n",
    "        # Create a dictionary for each article\n",
    "        article_dict = {}\n",
    "        \n",
    "        # Extract title using regex\n",
    "        title_search = re.search('<a href=\".+\">(.+)</a>', str(list_of_h2_elements[i]))\n",
    "        if title_search is not None:\n",
    "            article_dict['title'] = title_search.group(1)\n",
    "        \n",
    "        # Extract content using regex\n",
    "        content_search = re.search('<p>(.+)</p>', str(list_of_p_elements[i*2+1])) # assumes every other p element is content\n",
    "        if content_search is not None:\n",
    "            article_dict['content'] = content_search.group(1)\n",
    "        \n",
    "        # Add the article to the list\n",
    "        codeup_blog_articles.append(article_dict)\n",
    "    return codeup_blog_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21c9ad18-a3ef-4487-86f6-5c3daecd8210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Women in tech: Panelist Spotlight – Magdalena Rahn',\n",
       "  'content': 'Women in tech: Panelist Spotlight – Magdalena Rahn Codeup is hosting a Women in Tech Panel in honor of Women’s History...'},\n",
       " {'title': 'Women in tech: Panelist Spotlight – Rachel Robbins-Mayhill',\n",
       "  'content': 'Women in tech: Panelist Spotlight - Rachel Robbins-Mayhill Codeup is hosting a Women in Tech Panel in honor of Women’s...'},\n",
       " {'title': 'Women in Tech: Panelist Spotlight – Sarah Mellor',\n",
       "  'content': 'Women in tech: Panelist Spotlight - Sarah Mellor\\xa0 Codeup is hosting a Women in Tech Panel in honor of Women’s History...'},\n",
       " {'title': 'Women in Tech: Panelist Spotlight – Madeleine Capper',\n",
       "  'content': 'Women in tech: Panelist Spotlight - Madeleine Capper Codeup is hosting a Women in Tech Panel in honor of Women’s...'},\n",
       " {'title': 'Black Excellence in Tech: Panelist Spotlight – Wilmarie De La Cruz Mejia',\n",
       "  'content': 'Black excellence in tech: Panelist Spotlight - Wilmarie De La Cruz Mejia Codeup is hosting a Black Excellence in Tech...'},\n",
       " {'title': 'Black excellence in tech: Panelist Spotlight – Stephanie Jones',\n",
       "  'content': 'Black excellence in tech: Panelist Spotlight - Stephanie Jones Codeup is hosting our second Black Excellence in Tech...'},\n",
       " {}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_blog_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b8e90d-3492-448c-9e7a-d5dc86f0c5c7",
   "metadata": {},
   "source": [
    "2. News Articles\n",
    "\n",
    "We will now be scraping text data from inshorts, a website that provides a brief overview of many different topics.\n",
    "\n",
    "Write a function that scrapes the news articles for the following topics:\n",
    "\n",
    "Business\n",
    "Sports\n",
    "Technology\n",
    "Entertainment\n",
    "The end product of this should be a function named get_news_articles that returns a list of dictionaries, where each dictionary has this shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b69ba1-8858-4b21-a8bb-bee6845b0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'title': 'The article title',\n",
    "    'content': 'The article content',\n",
    "    'category': 'business' # for example\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3006b1e5-489e-460e-bbc4-0b6c9ba727dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_articles():\n",
    "    # The base url\n",
    "    base_url = \"https://inshorts.com/en/read/\"\n",
    "    \n",
    "    # List of news categories we're interested in\n",
    "    categories = [\"business\", \"sports\", \"technology\", \"entertainment\"]\n",
    "\n",
    "    # List to store the news articles\n",
    "    news_articles = []\n",
    "\n",
    "    # Loop over each category\n",
    "    for category in categories:\n",
    "        # Fetch the HTML of the page\n",
    "        response = requests.get(base_url + category)\n",
    "\n",
    "        # If the GET request is successful, the status code will be 200\n",
    "        if response.status_code == 200:\n",
    "            # Get the content of the response\n",
    "            page_content = response.content\n",
    "\n",
    "            # Create a BeautifulSoup object and specify the parser\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "            # Find all the news cards on the page\n",
    "            news_cards = soup.find_all('div', class_=['news-card'])\n",
    "\n",
    "            # Loop over each news card\n",
    "            for card in news_cards:\n",
    "                # Get the title of the news\n",
    "                title = card.find('span', itemprop='headline').string\n",
    "\n",
    "                # Get the content of the news\n",
    "                content = card.find('div', itemprop='articleBody').string\n",
    "\n",
    "                # Add the news article to the list\n",
    "                news_articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"content\": content,\n",
    "                    \"category\": category\n",
    "                })\n",
    "\n",
    "    # Return the list of news articles\n",
    "    return news_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea876a5-25b4-4719-b0b8-cc00fe897f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_news_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face5581-7a52-4279-a76b-9e347ee2439d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hints:\n",
    "\n",
    "- Start by inspecting the website in your browser. Figure out which elements will be useful.\n",
    "- Start by creating a function that handles a single article and produces a dictionary like the one above.\n",
    "- Next create a function that will find all the articles on a single page and call the function you created in the last step for every article on the page.\n",
    "- Now create a function that will use the previous two functions to scrape the articles from all the pages that you need, and do any - additional processing that needs to be done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cad505e6-7fe4-4fb1-90d6-e10cfff94ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_articles(use_cache=True):\n",
    "    # Define the filename for the cache file\n",
    "    cache_file = 'news_articles.csv'\n",
    "\n",
    "    # Check if we should use the cache file\n",
    "    if use_cache and os.path.exists(cache_file):\n",
    "        # Read the data from the cache file\n",
    "        news_articles = pd.read_csv(cache_file).to_dict('records')\n",
    "    else:\n",
    "        # The base url\n",
    "        base_url = \"https://inshorts.com/en/read/\"\n",
    "        \n",
    "        # List of news categories we're interested in\n",
    "        categories = [\"business\", \"sports\", \"technology\", \"entertainment\"]\n",
    "\n",
    "        # List to store the news articles\n",
    "        news_articles = []\n",
    "\n",
    "        # Loop over each category\n",
    "        for category in categories:\n",
    "            # Fetch the HTML of the page\n",
    "            response = requests.get(base_url + category)\n",
    "\n",
    "            # If the GET request is successful, the status code will be 200\n",
    "            if response.status_code == 200:\n",
    "                # Get the content of the response\n",
    "                page_content = response.content\n",
    "\n",
    "                # Create a BeautifulSoup object and specify the parser\n",
    "                soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "                # Find all the news cards on the page\n",
    "                news_cards = soup.find_all('div', class_=['news-card'])\n",
    "\n",
    "                # Loop over each news card\n",
    "                for card in news_cards:\n",
    "                    # Get the title of the news\n",
    "                    title = card.find('span', itemprop='headline').string\n",
    "\n",
    "                    # Get the content of the news\n",
    "                    content = card.find('div', itemprop='articleBody').string\n",
    "\n",
    "                    # Add the news article to the list\n",
    "                    news_articles.append({\n",
    "                        \"title\": title,\n",
    "                        \"content\": content,\n",
    "                        \"category\": category\n",
    "                    })\n",
    "\n",
    "        # Save the data to the cache file\n",
    "        pd.DataFrame(news_articles).to_csv(cache_file, index=False)\n",
    "\n",
    "    # Return the list of news articles\n",
    "    return news_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c73a05-782e-4ab7-b2ef-50efd330f8a1",
   "metadata": {},
   "source": [
    "- Bonus: cache the data\n",
    "\n",
    "Write your code such that the acquired data is saved locally in some form or fashion. Your functions that retrieve the data should prefer to read the local data instead of having to make all the requests everytime the function is called. Include a boolean flag in the functions to allow the data to be acquired \"fresh\" from the actual sources (re-writing your local cache)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f500d60d-bdb4-4b3b-b038-222ed5005766",
   "metadata": {},
   "source": [
    "# using sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f0face1-d3e5-48ef-b09b-cea737fd46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9205f5b3-4768-4ecd-9336-9afd74e74f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n"
     ]
    }
   ],
   "source": [
    "print('Hello')\n",
    "time.sleep(5)\n",
    "print('World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd04c257-4e02-4b9f-b841-d23c08af4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.nytimes.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aef3e0e-d30c-4004-b578-43e73e194864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4733a3c7-d7a7-449a-b445-6d2ae2d39ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d8166-e351-467e-9dbe-5774c99b2b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
